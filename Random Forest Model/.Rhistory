testFeature <- extractFeatures(test)
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season",
"holiday",
"workingday",
"weather",
"temp",
"atemp",
"humidity",
"windspeed",
"hour")
data$hour <- hour(ymd_hms(data$datetime))
return(data[,features])
}
trainFea <- extractFeatures(train)
testFea  <- extractFeatures(test)
submission <- data.frame(datetime=test$datetime, count=NA)
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
# extract basic number (y/m/d h) via function with ymd_hms
for (i_year in unique(year(ymd_hms(test$datetime)))) {
for (i_month in unique(month(ymd_hms(test$datetime)))) {
# Write process to the log with Random Forest
# Write process to the log with Random Forest
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFea <- extractFeatures(train)
testFea <- extractFeatures(test)
testFea <- extractFeatures(test)
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFeature <- extractFeatures(train)
testFea <- extractFeatures(test)
submission <- data.frame(datatime = test$datatime,
#                         season = test$season,
#                         holiday = test$holiday,
#                         workingday = test$workingday,
#                         weather = test$weather,
#                         temp = test$temp,
#                         atemp = test$atemp,
#                         humidity = test$humidity,
#                         windspeed = test$windspeed,
count = NA
)
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
# extract basic number (y/m/d h) via function with ymd_hms
for (i_year in unique(year(ymd_hms(test$datetime)))) {
for (i_month in unique(month(ymd_hms(test$datetime)))) {
# Write process to the log with Random Forest
# Write process to the log with Random Forest
# Write process to the log with Random Forest
# Write process to the log with Random Forest
# Write process to the log with Random Forest
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFeature <- extractFeatures(train)
testFeature <- extractFeatures(test)
submission <- data.frame(datatime = test$datatime,
#                         season = test$season,
#                         holiday = test$holiday,
#                         workingday = test$workingday,
#                         weather = test$weather,
#                         temp = test$temp,
#                         atemp = test$atemp,
#                         humidity = test$humidity,
#                         windspeed = test$windspeed,
count = NA
)
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
# extract basic number (y/m/d h) via function with ymd_hms
for (i_year in unique(year(ymd_hms(test$datetime)))) {
for (i_month in unique(month(ymd_hms(test$datetime)))) {
# Write process to the log with Random Forest
cat("Year: ", i_year, "\tMonth: ", i_month, "\n")
testLocs   <- year(ymd_hms(test$datetime)) == i_year & month(ymd_hms(test$datetime)) == i_month
testSubset <- test[testLocs,]
trainLocs  <- ymd_hms(train$datetime) <= min(ymd_hms(testSubset$datetime))
rf <- randomForest(extractFeatures(train[trainLocs,]), train[trainLocs,"count"], ntree = 100)
submission[testLocs, "count"] <- predict(rf, extractFeatures(testSubset))
}
}
write.csv(submission, file = "/output/submission_rf_output.csv", row.names = FALSE)
quit()
help()
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFeature <- extractFeatures(train)
testFeature <- extractFeatures(test)
trainFeature <- extractFeatures(train)
testFeature <- extractFeatures(test)
trainFea <- extractFeatures(train)
function (x, name, value)
{
cl <- oldClass(x)
class(x) <- NULL
nrows <- .row_names_info(x, 2L)
if (!is.null(value)) {
N <- NROW(value)
if (N > nrows)
stop(sprintf(ngettext(N, "replacement has %d row, data has %d",
"replacement has %d rows, data has %d"), N,
nrows), domain = NA)
if (N < nrows)
if (N > 0L && (nrows%%N == 0L) && length(dim(value)) <=
1L)
value <- rep(value, length.out = nrows)
else stop(sprintf(ngettext(N, "replacement has %d row, data has %d",
"replacement has %d rows, data has %d"), N,
nrows), domain = NA)
testFea <- extractFeatures(test)
submission <- data.frame(datatime = test$datatime,
#                         season = test$season,
#                         holiday = test$holiday,
quit
quit()
quit
quit()
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFea <- extractFeatures(train)
function (x, name, value)
{
cl <- oldClass(x)
quit
quit()
quit()
q()
quit()
quit()
help()
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
recover()
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFea <- extractFeatures(train)
testFea <- extractFeatures(test)
submission <- data.frame(datatime = test$datatime,
#                         season = test$season,
#                         holiday = test$holiday,
#                         workingday = test$workingday,
#                         weather = test$weather,
#                         temp = test$temp,
#                         atemp = test$atemp,
#                         windspeed = test$windspeed,
#                         humidity = test$humidity,
count = NA
)
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
# extract basic number (y/m/d h) via function with ymd_hms
for (i_year in unique(year(ymd_hms(test$datetime)))) {
for (i_month in unique(month(ymd_hms(test$datetime)))) {
# Write process to the log with Random Forest
cat("Year: ", i_year, "\tMonth: ", i_month, "\n")
testLocs   <- year(ymd_hms(test$datetime)) == i_year & month(ymd_hms(test$datetime)) == i_month
testSubset <- test[testLocs,]
trainLocs  <- ymd_hms(train$datetime) <= min(ymd_hms(testSubset$datetime))
rf <- randomForest(extractFeatures(train[trainLocs,]), train[trainLocs,"count"], ntree = 100)
submission[testLocs, "count"] <- predict(rf, extractFeatures(testSubset))
}
}
write.csv(submission, file = "/output/submission_rf_output.csv", row.names = FALSE)
browser()
browser(1)
q
quit
quit()
debuggingState(on=FALSE)
cl <- oldClass(x)
class(x) <- NULL
nrows <- .row_names_info(x, 2L)
if (!is.null(value)) {
N <- NROW(value)
if (N > nrows)
stop(sprintf(ngettext(N, "replacement has %d row, data has %d",
"replacement has %d rows, data has %d"), N,
nrows), domain = NA)
if (N < nrows)
if (N > 0L && (nrows%%N == 0L) && length(dim(value)) <=
1L)
value <- rep(value, length.out = nrows)
else stop(sprintf(ngettext(N, "replacement has %d row, data has %d",
"replacement has %d rows, data has %d"), N,
nrows), domain = NA)
if (is.atomic(value) && !is.null(names(value)))
names(value) <- NULL
}
x[[name]] <- value
class(x) <- cl
return(x)
}
source('E:/R Workstation/Random Forest Model/BicyleRF.R')
quit
quit()
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season","holiday","workingday","weather","temp","atemp","humidity","windspeed","hour")
data$hour <- hour(ymd_hms(data$datatime))
return(data[,features])
}
# input and output normalized
trainFea <- extractFeatures(train)
function (x, name, value)
{
cl <- oldClass(x)
class(x) <- NULL
nrows <- .row_names_info(x, 2L)
if (!is.null(value)) {
N <- NROW(value)
if (N > nrows)
stop(sprintf(ngettext(N, "replacement has %d row, data has %d",
"replacement has %d rows, data has %d"), N,
nrows), domain = NA)
if (N < nrows)
if (N > 0L && (nrows%%N == 0L) && length(dim(value)) <=
1L)
value <- rep(value, length.out = nrows)
else stop(sprintf(ngettext(N, "replacement has %d row, data has %d",
"replacement has %d rows, data has %d"), N,
nrows), domain = NA)
if (is.atomic(value) && !is.null(names(value)))
names(value) <- NULL
}
x[[name]] <- value
class(x) <- cl
return(x)
}
debuggingState(on=FALSE)
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season",
"holiday",
"workingday",
"weather",
"temp",
"atemp",
"humidity",
"windspeed",
"hour")
data$hour <- hour(ymd_hms(data$datetime))
return(data[,features])
}
# input and output normalized
trainFeature <- extractFeatures(train)
testFeature <- extractFeatures(test)
submission <- data.frame(datatime = test$datatime,
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season",
"holiday",
"workingday",
"weather","temp",
"atemp",
"humidity",
"windspeed",
"hour")
data$hour <- hour(ymd_hms(data$datetime))
return(data[,features])
}
# input and output normalized
trainFeature <- extractFeatures(train)
testFeature <- extractFeatures(test)
submission <- data.frame(datatime = test$datetime,
season = test$season,
holiday = test$holiday,
workingday = test$workingday,
weather = test$weather,
temp = test$temp,
atemp = test$atemp,
humidity = test$humidity,
windspeed = test$windspeed,
count = NA
)
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
# extract basic number (y/m/d h) via function with ymd_hms
for (i_year in unique(year(ymd_hms(test$datetime)))) {
for (i_month in unique(month(ymd_hms(test$datetime)))) {
# Write process to the log with Random Forest
cat("Year: ", i_year, "\tMonth: ", i_month, "\n")
testLocs   <- year(ymd_hms(test$datetime)) == i_year & month(ymd_hms(test$datetime)) == i_month
testSubset <- test[testLocs,]
trainLocs  <- ymd_hms(train$datetime) <= min(ymd_hms(testSubset$datetime))
rf <- randomForest(extractFeatures(train[trainLocs,]), train[trainLocs,"count"], ntree = 100)
submission[testLocs, "count"] <- predict(rf, extractFeatures(testSubset))
}
}
write.csv(submission, file = "/output/submission_rf_output.csv", row.names = FALSE)
write.csv(submission, file = "output/submission_rf_output.csv", row.names = FALSE)
# This R script creates a sample submission via Random Forests
# and plots the freature importance from the trained model
library(ggplot2)
library(lubridate)
library(randomForest)
set.seed(1)
setwd("E:/R Workstation/Random Forest Model/")
train <- read.csv("input/train.csv")
test <- read.csv("input/test.csv")
# Write to the log
cat(sprintf("Training set has %d rows and %d columns\n", nrow(train), ncol(train)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(test), ncol(test)))
library(randomForest)
extractFeatures <- function(data) {
features <- c("season",
"holiday",
"workingday",
"weather",
"temp",
"atemp",
"humidity",
"windspeed",
"hour")
data$hour <- hour(ymd_hms(data$datetime))
return(data[,features])
}
# input and output normalized
trainFeature <- extractFeatures(train)
testFeature <- extractFeatures(test)
submission <- data.frame(datatime = test$datetime,
season = test$season,
holiday = test$holiday,
workingday = test$workingday,
weather = test$weather,
temp = test$temp,
atemp = test$atemp,
humidity = test$humidity,
windspeed = test$windspeed,
count = NA
)
# We only use past data to make predictions on the test set
# so we train a new model for each test set cutoff point
# extract basic number (y/m/d h) via function with ymd_hms
for (i_year in unique(year(ymd_hms(test$datetime)))) {
for (i_month in unique(month(ymd_hms(test$datetime)))) {
# Write process to the log with Random Forest
cat("Year: ", i_year, "\tMonth: ", i_month, "\n")
testLocs   <- year(ymd_hms(test$datetime)) == i_year & month(ymd_hms(test$datetime)) == i_month
testSubset <- test[testLocs,]
trainLocs  <- ymd_hms(train$datetime) <= min(ymd_hms(testSubset$datetime))
rf <- randomForest(extractFeatures(train[trainLocs,]), train[trainLocs,"count"], ntree = 100)
submission[testLocs, "count"] <- predict(rf, extractFeatures(testSubset))
}
}
write.csv(submission, file = "output/submission_rf_output.csv", row.names = FALSE)
# Random Forest Importance
rf <- randomForest(extractFeatures(train), train$count, ntree=100, importance=TRUE)
imp <- importance(rf, type = 1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])
# plot the features with library(ggplot)
p <- ggplot(featureImportance, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "#0099FF") +
coord_flip() +
theme_light(base_size = 20) +
xlab("Importance") +
ylab("") +
ggtitle("Random Forest Feature Importance\n") +
theme(plot.title = element_text(size = 18))
ggsave("output/importance_output.png", p)
